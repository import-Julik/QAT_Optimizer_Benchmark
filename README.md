# QAT Optimizer Benchmark (LSQ)
### Benchmarking Optimization Methods for Quantization-Aware Training with LSQ on ESPCN (DIV2K) and ResNet-18 (ThuDogs)

---

## Описание проекта

Этот репозиторий содержит исследование влияния различных оптимизаторов на качество моделей при **Quantization-Aware Training (QAT)** с использованием **Learned Step Size Quantization (LSQ)**.

Проект включает две задачи:

- **Super-Resolution**
  - модель: **ESPCN (LSQ)**
  - датасет: **DIV2K**
  - метрика: **PSNR**

- **Классификация**
  - модель: **ResNet-18 (LSQ)**
  - датасет: **ThuDogs**
  - метрика: **Accuracy**

Цель — понять устойчивость разных оптимизаторов к квантизационному шуму.

---

# Оптимизаторы в эксперименте (10 шт.)

| № | Оптимизатор |
|---|-------------|
| 1 | Adam | 
| 2 | AdamW | 
| 3 | RMSProp |
| 4 | AdaGrad | 
| 5 | Adadelta | 
| 6 | SGD | 
| 7 | Nesterov |
| 8 | Lion | 
| 9 | SGD + Gradient Clipping | 
|10 | Adaptive Gradient Clipping (AGC) | 

---

# Результаты: Super-Resolution (DIV2K, ESPCN-LSQ)

Метрика: PSNR  (среднее по 10 запускам)

| Оптимизатор | PSNR |
|-------------|-------|
| **AdamW** | **21.7968** |
| **Adam** | **21.6762** |
| RMSprop | 21.3211 |
| AdaGrad | 20.3651 |
| Lion | 17.7479 |
| Nesterov | 14.2598 |
| SGD_GCM | 13.1684 |
| SGD | 13.1684 |
| AGC_SGD | 6.4209 |
| Adadelta | 6.2298 |

---

# Результаты: Classification (ThuDogs, ResNet18-LSQ)

Метрика: Accuracy ↑ (среднее по 10 запускам)

| Оптимизатор | Accuracy |
|-------------|----------|
| **AdaGrad** | **4.615%** |
| Adam | 3.846% |
| AdamW | 3.076% |
| AGC_SGD | 2.307% |
| Adadelta | 2.307% |
| Lion | 2.307% |
| Nesterov | 2.307% |
| RMSprop | 2.307% |
| SGD | 2.307% |
| SGD_GCM | 1.538% |

---

# Интерпретация результатов

###  1. Адаптивные оптимизаторы устойчивее к шуму LSQ  
AdamW, Adam и RMSprop показали лучшие результаты в обеих задачах.  
Это объясняется тем, что:

- LSQ создаёт высокодисперсный градиент  
- адаптивные методы лучше нормализуют масштаб градиента  
- AdamW стабилизирует шаг за счёт weight decay

###  2. SGD-подобные методы сильно деградируют  
SGD, Nesterov и Lion плохо справляются, т.к.:

- чувствительны к масштабам шумных градиентов
- у LSQ низкая точность градиента → плохая сходимость
- momentum загрязняется квантовыми артефактами

### 3. Шумоустойчивые методы (AGC, GCM) работают хуже всех 

Причина:

- AGC/GCM разработаны для стабилизации *огромных градиентов*
- в LSQ разложение градиента другое → они режут полезный сигнал
- модель входит в режим хронического недообучения

---

# Ограничения исследования

- ограниченные вычисленияя (6 эпох × 10 запусков)
- фиксированные архитектуры (ESPCN и ResNet18)
- фиксированная схема LSQ
- ограниченный поиск по batch size и learning rate

---

# Выводы

- Адаптивные методы (AdamW, Adam) наиболее устойчивы к шумным LSQ-градиентам  
- SGD-подобные методы показывают резкое падение качества  
- Низкие абсолютные метрики — это нормальная характеристика QAT с LSQ на ранних эпохах  
- Исследование валидно, т.к. сравнение проводится в **равных условиях**

